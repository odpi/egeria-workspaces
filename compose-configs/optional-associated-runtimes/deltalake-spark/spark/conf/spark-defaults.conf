# Spark defaults for Delta Lake + Hive Metastore + MinIO (S3A)
# These settings are used by the Spark containers in this compose stack.

# Delta Lake integration
spark.sql.extensions                       org.apache.spark.sql.delta.extensions.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog            org.apache.spark.sql.delta.catalog.DeltaCatalog

# Use Hive catalog backed by external Hive Metastore service
spark.sql.catalogImplementation            hive
spark.hadoop.hive.metastore.uris           thrift://hive-metastore:9083

# Where Spark keeps its local warehouse (for managed tables stored on local FS)
spark.sql.warehouse.dir                   /opt/spark/warehouse

# S3A connector settings to point to MinIO inside this compose network
spark.hadoop.fs.s3a.endpoint              http://minio:9000
spark.hadoop.fs.s3a.access.key            ${MINIO_ROOT_USER}
spark.hadoop.fs.s3a.secret.key            ${MINIO_ROOT_PASSWORD}
spark.hadoop.fs.s3a.path.style.access     true
spark.hadoop.fs.s3a.connection.ssl.enabled false
spark.hadoop.fs.s3a.impl                  org.apache.hadoop.fs.s3a.S3AFileSystem

# Jars (downloaded automatically on startup)
# - delta-spark for Spark 3.5.x (Scala 2.12)
# - hadoop-aws to enable S3A
# Postgres JDBC is not required by Spark itself here (Hive connects to PG), but harmless if added.
spark.jars.packages                       io.delta:delta-spark_2.12:3.2.0,org.apache.hadoop:hadoop-aws:3.3.4

# Reduce noisy S3 logging
spark.hadoop.fs.s3a.connection.establish.timeout    5000
spark.hadoop.fs.s3a.connection.maximum              200
